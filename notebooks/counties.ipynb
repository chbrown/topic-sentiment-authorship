{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import IPython\n",
      "import os\n",
      "import subprocess\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from tsa.science import numpy_ext as npx\n",
      "\n",
      "import itertools\n",
      "from collections import Counter\n",
      "\n",
      "from sklearn import metrics, cross_validation\n",
      "from sklearn import linear_model\n",
      "from sklearn import naive_bayes\n",
      "from sklearn import svm\n",
      "\n",
      "from tsa import stdout, stderr, root\n",
      "from tsa.lib import tabular, datetime_extra\n",
      "from tsa.lib.timer import Timer\n",
      "from tsa.models import Source, Document, create_session\n",
      "from tsa.science import features, models, timeseries\n",
      "from tsa.science.corpora import MulticlassCorpus\n",
      "from tsa.science.plot import plt, figure_path, distinct_styles, ticker\n",
      "from tsa.science.summarization import metrics_dict, metrics_summary\n",
      "\n",
      "import geo\n",
      "import geo.types\n",
      "import geo.shapefile.reader\n",
      "import geo.shapefile.writer\n",
      "import geojson\n",
      "\n",
      "from tsa import logging\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "head = lambda x: x[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "session = create_session()\n",
      "documents = session.query(Document).join(Source).\\\n",
      "    filter(Source.name == 'sb5b').all()\n",
      "\n",
      "full_corpus = MulticlassCorpus(documents)\n",
      "full_corpus.apply_labelfunc(lambda doc: doc.label or 'Unlabeled')\n",
      "full_corpus.extract_features(lambda doc: doc.document, features.ngrams,\n",
      "    ngram_max=2, min_df=2, max_df=1.0)\n",
      "\n",
      "polar_classes = [full_corpus.class_lookup[label] for label in ['For', 'Against']]\n",
      "polar_indices = np.in1d(full_corpus.y, polar_classes)\n",
      "labeled_corpus = full_corpus.subset(polar_indices)\n",
      "\n",
      "logreg_model = linear_model.LogisticRegression(fit_intercept=True, penalty='l2')\n",
      "logreg_model.fit(labeled_corpus.X, labeled_corpus.y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
        "          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001)"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# lon = x = easting; lat = y = northing\n",
      "geolocated_indices = np.array([\n",
      "    (doc.details.get('Longitude') is not None and doc.details.get('Latitude') is not None)\n",
      "    for doc in full_corpus.data])\n",
      "print 'Number of geo-located tweets: {:d} ({:.2%})'.format(\n",
      "    geolocated_indices.sum(), geolocated_indices.mean())\n",
      "\n",
      "geolocated_corpus = full_corpus.subset(geolocated_indices)\n",
      "geolocated_pred_y = logreg_model.predict(geolocated_corpus.X)\n",
      "# geolocated_pred_proba = logreg_model.predict_proba(geolocated_corpus.X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of geo-located tweets: 3769 (3.53%)\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# countyp020.shp has fields like: 'PERIMETER': 1.82, 'STATE_FIPS': '39',\n",
      "#   'AREA': 0.199, 'COUNTY': 'Ashtabula County', 'STATE': 'OH', 'FIPS': '39007',\n",
      "#   'COUNTYP020': 2539, 'SQUARE_MIL': 710.321}  \n",
      "counties_filepath = '/Users/chbrown/corpora-public/census-shapefiles/countyp020/countyp020.shp'\n",
      "# counties_filepath = '/Users/chbrown/corpora-public/census-shapefiles/ESRI-UScounties/UScounties.shp'\n",
      "shapefile_reader = geo.shapefile.reader.Reader(counties_filepath)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_ohio_counties(shapefile_reader):\n",
      "    for record, shape in zip(shapefile_reader.records(), shapefile_reader.shapes()):\n",
      "        attributes = dict(zip(shapefile_reader.field_names, record))\n",
      "        state = attributes['STATE']\n",
      "        county = attributes['COUNTY'].replace('County', '').strip()\n",
      "        if state == 'OH' and county:\n",
      "            yield county, attributes, shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "grouped = itertools.groupby(sorted(get_ohio_counties(shapefile_reader), key=head), head)\n",
      "\n",
      "features = []\n",
      "for county, group in grouped:\n",
      "    group = list(group)\n",
      "    #print county, len(group)\n",
      "    if len(group) == 1:\n",
      "        geometry = group[0][2].__geo_interface__\n",
      "    else:\n",
      "        geometry = geojson.MultiPolygon([item[2].__geo_interface__['coordinates'] for item in group])\n",
      "    properties = dict(county=county, FIPS=group[0][1]['FIPS'], area_sq_mi=group[0][1]['SQUARE_MIL'])\n",
      "    #bbox = geo.types.BoundingBox(*group[0][2].bbox)\n",
      "    bbox = None\n",
      "    features += [geo.types.Feature(geometry, properties, id=county, bbox=bbox)]\n",
      "\n",
      "feature_collection = geo.types.FeatureCollection(features)\n",
      "out_of_state = geo.types.Feature(dict(type=None), dict(FIPS=None), id='Out-of-state')\n",
      "feature_collection.features += [out_of_state]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "counties = [\n",
      "            ('Ottawa', feature_collection['Ottawa']),\n",
      "            ('Franklin', feature_collection['Franklin']),\n",
      "           ]\n",
      "cities = [\n",
      "          ('Austin', (-97.75, 30.25)),\n",
      "          ('Columbus', (-82.98, 39.98)),\n",
      "          ('Port Clinton', (-82.9433, 41.5063)), # in ottawa county, which has islands\n",
      "         ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# for feature in feature_collection.features:\n",
      "#     print feature.id, feature.bbox\n",
      "# ottawa = feature_collection['Ottawa']\n",
      "# ottawa.contains(-82.9433, 41.5063)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#parts = shape.parts.tolist() + [len(shape.points)]\n",
      "#polygons = [shape.points[i:j] for i, j in zip(parts, parts[1:])]\n",
      "print 'Testing county coverage...'\n",
      "for county, feature in counties:\n",
      "    for city, (lon, lat) in cities:\n",
      "        print '{:s} contains {:s}? {:s}'.format(\n",
      "            county, city, str(feature.contains(lon, lat)))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Testing county coverage...\n",
        "Ottawa contains Austin? False\n",
        "Ottawa contains Columbus? False\n",
        "Ottawa contains Port Clinton? True\n",
        "Franklin contains Austin? False\n",
        "Franklin contains Columbus? True\n",
        "Franklin contains Port Clinton? False\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# add population data\n",
      "fips2county = dict((feature.properties['FIPS'], feature.id) for feature in feature_collection.features)\n",
      "import csv\n",
      "reader = csv.DictReader(open('/Users/chbrown/corpora/ohio/census/DataSet.txt'))\n",
      "for row in reader:\n",
      "    fips = row['fips']\n",
      "    # POP010210 = Population, 2010\n",
      "    population = row['POP010210']\n",
      "    ohio_county = fips2county.get(fips)\n",
      "    if ohio_county:\n",
      "        county_feature = feature_collection[ohio_county]\n",
      "        county_feature.properties['population'] = int(population)\n",
      "        \n",
      "total_pop = sum(feature.properties.get('population', 0) for feature in feature_collection.features)\n",
      "print total_pop, 'vs. Wikipedia\\'s \"2010 11,536,504\"'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "11536504 vs. Wikipedia's \"2010 11,536,504\"\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def location(document, feature_collection):\n",
      "    lon = document.details.get('Longitude')\n",
      "    lat = document.details.get('Latitude')\n",
      "    features = list(feature_collection.features_containing(lon, lat))\n",
      "    # country_match = countries.first_area_containing(lon, lat)\n",
      "    in_state = len(features) == 1\n",
      "    return features[0].id if in_state else 'Out-of-state'\n",
      "\n",
      "# for feature in feature_collection.features:\n",
      "#     print feature.id, feature.properties\n",
      "\n",
      "from collections import defaultdict\n",
      "\n",
      "# geolocated_counties = np.array([location(document) for document in geolocated_corpus.data])\n",
      "geolocated_pred_labels = geolocated_corpus.labels[geolocated_pred_y]\n",
      "for pred_label, document in zip(geolocated_pred_labels, geolocated_corpus.data):\n",
      "    feature_id = location(document, feature_collection)\n",
      "    feature = feature_collection[feature_id]\n",
      "    tweets = feature.properties.setdefault('tweets', {})\n",
      "    tweets[pred_label] = tweets.get(pred_label, 0) + 1\n",
      "    tweets['Total'] = tweets.get('Total', 0) + 1\n",
      "    if document.label in ['For', 'Against']:\n",
      "        labeled_tweets = feature.properties.setdefault('labeled_tweets', {})\n",
      "        labeled_tweets[pred_label] = labeled_tweets.get(pred_label, 0) + 1\n",
      "        labeled_tweets['Total'] = labeled_tweets.get('Total', 0) + 1\n",
      "    \n",
      "# Counter(geolocated_counties)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# geojson_path = os.path.join(root, 'static', 'oh-counties.geo.json')\n",
      "geojson_encoder = geo.types.GeoEncoder(indent=None)\n",
      "geojson_string = encoder.encode(feature_collection)\n",
      "# with open(geojson_path, 'w') as geojson_fd:\n",
      "#     geojson_fd.write()\n",
      "topojson_path = os.path.join(root, 'static', 'oh-counties.topo.json')\n",
      "from subprocess import PIPE\n",
      "proc = subprocess.Popen(['topojson', '--properties', '--out', topojson_path],\n",
      "                        stdin=PIPE, stdout=PIPE, stderr=PIPE)\n",
      "stdout, stderr = proc.communicate(geojson_string)\n",
      "print 'Wrote feature_collection to GeoJSON and converted to TopoJSON'\n",
      "print stdout, stderr"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Wrote feature_collection to GeoJSON and converted to TopoJSON\n",
        " bounds: -84.81993103027344 38.40250015258789 -80.51771545410156 41.978248596191406 (spherical)\n",
        "pre-quantization: 0.478m (0.00000430\u00b0) 0.398m (0.00000358\u00b0)\n",
        "topology: 262 arcs, 3840 points\n",
        "post-quantization: 47.8m (0.000430\u00b0) 39.8m (0.000358\u00b0)\n",
        "prune: retained 262 / 262 arcs (100%)\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 29
    }
   ],
   "metadata": {}
  }
 ]
}