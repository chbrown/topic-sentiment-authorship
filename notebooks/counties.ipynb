{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import IPython\n",
      "import os\n",
      "import subprocess\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from tsa.science import numpy_ext as npx\n",
      "\n",
      "import itertools\n",
      "from collections import Counter\n",
      "\n",
      "from sklearn import metrics, cross_validation\n",
      "from sklearn import linear_model\n",
      "from sklearn import naive_bayes\n",
      "from sklearn import svm\n",
      "\n",
      "from tsa import stdout, stderr, root\n",
      "from tsa.lib import tabular, datetime_extra\n",
      "from tsa.lib.timer import Timer\n",
      "from tsa.models import Source, Document, create_session\n",
      "from tsa.science import features, models, timeseries\n",
      "from tsa.science.corpora import MulticlassCorpus\n",
      "from tsa.science.plot import plt, figure_path, distinct_styles, ticker\n",
      "from tsa.science.summarization import metrics_dict, metrics_summary\n",
      "\n",
      "import geo\n",
      "import geo.types\n",
      "import geo.shapefile.reader\n",
      "import geo.shapefile.writer\n",
      "import geojson\n",
      "\n",
      "from tsa import logging\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "head = lambda x: x[0]\n",
      "\n",
      "# lon = x = easting; lat = y = northing"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "session = create_session()\n",
      "documents = session.query(Document).join(Source).\\\n",
      "    filter(Source.name == 'sb5b').all()\n",
      "\n",
      "full_corpus = MulticlassCorpus(documents)\n",
      "full_corpus.apply_labelfunc(lambda doc: doc.label or 'Unlabeled')\n",
      "full_corpus.extract_features(lambda doc: doc.document, features.ngrams,\n",
      "    ngram_max=2, min_df=2, max_df=1.0)\n",
      "\n",
      "def doc_has_lon_lat(doc):\n",
      "    return doc.details.get('Longitude') is not None and doc.details.get('Latitude') is not None\n",
      "\n",
      "geolocated_indices = np.array([doc_has_lon_lat(doc) for doc in full_corpus.data])\n",
      "labeled_geolocated_indices = np.array([doc_has_lon_lat(doc) and doc.label is not None\n",
      "                                       for doc in full_corpus.data])\n",
      "\n",
      "print 'Number of geo-located tweets: {:d} ({:.2%})'.format(\n",
      "    geolocated_indices.sum(), geolocated_indices.mean())\n",
      "print 'Number of labeled geo-located tweets: {:d} ({:.2%})'.format(\n",
      "    labeled_geolocated_indices.sum(), labeled_geolocated_indices.mean())\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of geo-located tweets: 3769 (3.53%)\n",
        "Number of labeled geo-located tweets: 247 (0.23%)\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "polar_classes = [full_corpus.class_lookup[label] for label in ['For', 'Against']]\n",
      "polar_indices = np.in1d(full_corpus.y, polar_classes)\n",
      "labeled_corpus = full_corpus.subset(polar_indices)\n",
      "\n",
      "logreg_model = linear_model.LogisticRegression(fit_intercept=True, penalty='l2')\n",
      "logreg_model.fit(labeled_corpus.X, labeled_corpus.y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
        "          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001)"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "geolocated_corpus = full_corpus.subset(geolocated_indices)\n",
      "geolocated_pred_y = logreg_model.predict(geolocated_corpus.X)\n",
      "# geolocated_pred_proba = logreg_model.predict_proba(geolocated_corpus.X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# countyp020.shp has fields like: 'PERIMETER': 1.82, 'STATE_FIPS': '39',\n",
      "#   'AREA': 0.199, 'COUNTY': 'Ashtabula County', 'STATE': 'OH', 'FIPS': '39007',\n",
      "#   'COUNTYP020': 2539, 'SQUARE_MIL': 710.321}  \n",
      "counties_filepath = '/Users/chbrown/corpora-public/census-shapefiles/countyp020/countyp020.shp'\n",
      "# counties_filepath = '/Users/chbrown/corpora-public/census-shapefiles/ESRI-UScounties/UScounties.shp'\n",
      "shapefile_reader = geo.shapefile.reader.Reader(counties_filepath)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_ohio_counties(shapefile_reader):\n",
      "    for record, shape in zip(shapefile_reader.records(), shapefile_reader.shapes()):\n",
      "        attributes = dict(zip(shapefile_reader.field_names, record))\n",
      "        state = attributes['STATE']\n",
      "        county = attributes['COUNTY'].replace('County', '').strip()\n",
      "        if state == 'OH' and county:\n",
      "            yield county, attributes, shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "grouped = itertools.groupby(sorted(get_ohio_counties(shapefile_reader), key=head), head)\n",
      "\n",
      "features = []\n",
      "for county, group in grouped:\n",
      "    group = list(group)\n",
      "    #print county, len(group)\n",
      "    if len(group) == 1:\n",
      "        geometry = group[0][2].__geo_interface__\n",
      "    else:\n",
      "        geometry = geojson.MultiPolygon([item[2].__geo_interface__['coordinates'] for item in group])\n",
      "    properties = dict(county=county, FIPS=group[0][1]['FIPS'], area_sq_mi=group[0][1]['SQUARE_MIL'])\n",
      "    #bbox = geo.types.BoundingBox(*group[0][2].bbox)\n",
      "    bbox = None\n",
      "    features += [geo.types.Feature(geometry, properties, id=county, bbox=bbox)]\n",
      "\n",
      "feature_collection = geo.types.FeatureCollection(features)\n",
      "out_of_state = geo.types.Feature(dict(type=None), dict(FIPS=None), id='Out-of-state')\n",
      "feature_collection.features += [out_of_state]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "counties = [\n",
      "            ('Ottawa', feature_collection['Ottawa']),\n",
      "            ('Franklin', feature_collection['Franklin']),\n",
      "           ]\n",
      "cities = [\n",
      "          ('Austin', (-97.75, 30.25)),\n",
      "          ('Columbus', (-82.98, 39.98)),\n",
      "          ('Port Clinton', (-82.9433, 41.5063)), # in ottawa county, which has islands\n",
      "         ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# for feature in feature_collection.features:\n",
      "#     print feature.id, feature.bbox\n",
      "# ottawa = feature_collection['Ottawa']\n",
      "# ottawa.contains(-82.9433, 41.5063)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#parts = shape.parts.tolist() + [len(shape.points)]\n",
      "#polygons = [shape.points[i:j] for i, j in zip(parts, parts[1:])]\n",
      "print 'Testing county coverage...'\n",
      "for county, feature in counties:\n",
      "    for city, (lon, lat) in cities:\n",
      "        print '{:s} contains {:s}? {:s}'.format(\n",
      "            county, city, str(feature.contains(lon, lat)))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Testing county coverage...\n",
        "Ottawa contains Austin? False\n",
        "Ottawa contains Columbus? False\n",
        "Ottawa contains Port Clinton? True\n",
        "Franklin contains Austin? False\n",
        "Franklin contains Columbus? True\n",
        "Franklin contains Port Clinton? False\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# add population data\n",
      "fips2county = dict((feature.properties['FIPS'], feature.id) for feature in feature_collection.features)\n",
      "import csv\n",
      "reader = csv.DictReader(open('/Users/chbrown/corpora/ohio/census/DataSet.txt'))\n",
      "for row in reader:\n",
      "    fips = row['fips']\n",
      "    # POP010210 = Population, 2010\n",
      "    population = row['POP010210']\n",
      "    ohio_county = fips2county.get(fips)\n",
      "    if ohio_county:\n",
      "        county_feature = feature_collection[ohio_county]\n",
      "        county_feature.properties['population'] = int(population)\n",
      "\n",
      "def feature_population(feature):    \n",
      "    return feature.properties.get('population', 0)\n",
      "for feature in sorted(feature_collection.features, key=feature_population):\n",
      "    print feature.id, feature_population(feature)\n",
      "        \n",
      "total_pop = sum(feature.properties.get('population', 0) for feature in feature_collection.features)\n",
      "print total_pop, 'vs. Wikipedia\\'s \"2010 11,536,504\"'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Out-of-state 0\n",
        "Vinton 13435\n",
        "Monroe 14642\n",
        "Noble 14645\n",
        "Morgan 15054\n",
        "Harrison 15864\n",
        "Paulding 19614\n",
        "Wyandot 22615\n",
        "Meigs 23770\n",
        "Henry 28215\n",
        "Adams 28550\n",
        "Pike 28709\n",
        "Van Wert 28744\n",
        "Carroll 28836\n",
        "Fayette 29030\n",
        "Hocking 29380\n",
        "Gallia 30934\n",
        "Hardin 32058\n",
        "Jackson 33225\n",
        "Putnam 34499\n",
        "Morrow 34827\n",
        "Perry 36058\n",
        "Coshocton 36901\n",
        "Williams 37642\n",
        "Defiance 39037\n",
        "Guernsey 40087\n",
        "Champaign 40097\n",
        "Mercer 40814\n",
        "Ottawa 41428\n",
        "Clinton 42040\n",
        "Preble 42270\n",
        "Holmes 42366\n",
        "Fulton 42698\n",
        "Madison 43435\n",
        "Highland 43589\n",
        "Crawford 43784\n",
        "Brown 44846\n",
        "Logan 45858\n",
        "Auglaize 45949\n",
        "Shelby 49423\n",
        "Union 52300\n",
        "Darke 52959\n",
        "Ashland 53139\n",
        "Pickaway 55698\n",
        "Seneca 56745\n",
        "Huron 59626\n",
        "Knox 60921\n",
        "Sandusky 60944\n",
        "Washington 61778\n",
        "Lawrence 62450\n",
        "Athens 64757\n",
        "Marion 66501\n",
        "Jefferson 69709\n",
        "Belmont 70400\n",
        "Hancock 74782\n",
        "Erie 77079\n",
        "Ross 78064\n",
        "Scioto 79499\n",
        "Muskingum 86074\n",
        "Tuscarawas 92582\n",
        "Geauga 93389\n",
        "Ashtabula 101497\n",
        "Miami 102506\n",
        "Allen 106331\n",
        "Columbiana 107841\n",
        "Wayne 114520\n",
        "Richland 124475\n",
        "Wood 125488\n",
        "Clark 138333\n",
        "Fairfield 146156\n",
        "Portage 161419\n",
        "Greene 161573\n",
        "Licking 166492\n",
        "Medina 172332\n",
        "Delaware 174214\n",
        "Clermont 197363\n",
        "Trumbull 210312\n",
        "Warren 212693\n",
        "Lake 230041\n",
        "Mahoning 238823\n",
        "Lorain 301356\n",
        "Butler 368130\n",
        "Stark 375586\n",
        "Lucas 441815\n",
        "Montgomery 535153\n",
        "Summit 541781\n",
        "Hamilton 802374\n",
        "Franklin 1163414\n",
        "Cuyahoga 1280122\n",
        "11536504 vs. Wikipedia's \"2010 11,536,504\"\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def location(document, feature_collection):\n",
      "    lon = document.details.get('Longitude')\n",
      "    lat = document.details.get('Latitude')\n",
      "    features = list(feature_collection.features_containing(lon, lat))\n",
      "    # country_match = countries.first_area_containing(lon, lat)\n",
      "    in_state = len(features) == 1\n",
      "    return features[0].id if in_state else 'Out-of-state'\n",
      "\n",
      "# for feature in feature_collection.features:\n",
      "#     print feature.id, feature.properties\n",
      "\n",
      "from collections import defaultdict\n",
      "\n",
      "# geolocated_counties = np.array([location(document) for document in geolocated_corpus.data])\n",
      "geolocated_pred_labels = geolocated_corpus.labels[geolocated_pred_y]\n",
      "for pred_label, document in zip(geolocated_pred_labels, geolocated_corpus.data):\n",
      "    feature_id = location(document, feature_collection)\n",
      "    feature = feature_collection[feature_id]\n",
      "    tweets = feature.properties.setdefault('tweets', {})\n",
      "    tweets[pred_label] = tweets.get(pred_label, 0) + 1\n",
      "    tweets['Total'] = tweets.get('Total', 0) + 1\n",
      "    if document.label in ['For', 'Against']:\n",
      "        labeled_tweets = feature.properties.setdefault('labeled_tweets', {})\n",
      "        labeled_tweets[pred_label] = labeled_tweets.get(pred_label, 0) + 1\n",
      "        labeled_tweets['Total'] = labeled_tweets.get('Total', 0) + 1\n",
      "    \n",
      "# Counter(geolocated_counties)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# geojson_path = os.path.join(root, 'static', 'oh-counties.geo.json')\n",
      "geojson_encoder = geo.types.GeoEncoder(indent=None)\n",
      "geojson_string = encoder.encode(feature_collection)\n",
      "# with open(geojson_path, 'w') as geojson_fd:\n",
      "#     geojson_fd.write()\n",
      "topojson_path = os.path.join(root, 'static', 'oh-counties.topo.json')\n",
      "from subprocess import PIPE\n",
      "proc = subprocess.Popen(['topojson', '--properties', '--out', topojson_path],\n",
      "                        stdin=PIPE, stdout=PIPE, stderr=PIPE)\n",
      "stdout, stderr = proc.communicate(geojson_string)\n",
      "print 'Wrote feature_collection to GeoJSON and converted to TopoJSON'\n",
      "print stdout, stderr"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Wrote feature_collection to GeoJSON and converted to TopoJSON\n",
        " bounds: -84.81993103027344 38.40250015258789 -80.51771545410156 41.978248596191406 (spherical)\n",
        "pre-quantization: 0.478m (0.00000430\u00b0) 0.398m (0.00000358\u00b0)\n",
        "topology: 262 arcs, 3840 points\n",
        "post-quantization: 47.8m (0.000430\u00b0) 39.8m (0.000358\u00b0)\n",
        "prune: retained 262 / 262 arcs (100%)\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 29
    }
   ],
   "metadata": {}
  }
 ]
}